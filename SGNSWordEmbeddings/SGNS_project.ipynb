{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SGNS Embedding Solvers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages graphframes:graphframes:0.5.0-spark2.1-s_2.11,com.databricks:spark-xml_2.11:0.4.1 pyspark-shell'\n",
    "\n",
    "from pyspark import SparkContext\n",
    "sc = SparkContext()\n",
    "from pyspark.sql import SQLContext, Row, Window\n",
    "from pyspark.sql.types import *\n",
    "import pyspark.sql.functions as func\n",
    "sqlContext = SQLContext(sc)\n",
    "\n",
    "from pyspark.ml.feature import Word2Vec\n",
    "from pyspark.mllib.linalg.distributed import CoordinateMatrix\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.util import bigrams, ngrams\n",
    "\n",
    "from datetime import datetime as dt\n",
    "from operator import add\n",
    "\n",
    "import gcsfs\n",
    "import re\n",
    "import numpy as np\n",
    "import string\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "sc.setCheckpointDir(\"gs://fin-bucket/tmp/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use English wiki as out corpus. The data is freely available to download and comes in XML files, which were processed using wiki extractor (a software package that can be found here https://github.com/attardi/wikiextractor) to extract the text from the articles. The extracted text from articles are written to documents are then stored in HDFS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<doc id=\"3943092\" url=\"https://en.wikipedia.org/wiki?curid=3943092\" title=\"Let\\'s Get It: Thug Motivation 101\">',\n",
       " \"Let's Get It: Thug Motivation 101\",\n",
       " '',\n",
       " 'Let\\'s Get It: Thug Motivation 101 is the major label debut studio album by American rapper Young Jeezy. It was released on July 26, 2005, by his indie record label Corporate Thugz, under the distribution from Def Jam. On the UK\\'s album release, it features the remixed version of \"Go Crazy\", featuring guest appearance from rapper Jay-Z.',\n",
       " '',\n",
       " 'The album was supported by 4 singles: \"And Then What\" featuring Mannie Fresh, \"Soul Survivor\" featuring Akon, \"Go Crazy\" and \"My Hood\".',\n",
       " '',\n",
       " 'The album debuted at #2 on the \"Billboard\" 200, selling 172,000 copies in the first week. On September 29, 2005, the album was certified Platinum by the Recording Industry Association of America (RIAA).',\n",
       " '',\n",
       " 'In 2015, hip hop writer Brooklyn Russell declared the album \"trap rap\\'s apotheosis\" while observing its impact: \"Working with only a handful of Shawty Redd beats and his naturally raspy voice, Atlanta native Young Jeezy would lay down the blueprint for an entire region of rappersâ€”virtually knocking big players like Lil Jon out of commission.\"']"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fpath = \"hdfs:///wiki/\"\n",
    "\n",
    "paths = \",\".join([f\"{fpath}{folder}/wiki_{file:0>2}\" \n",
    "                  for folder in {\"AA\"}#, \"AB\", \"AC\", \"AD\"} \n",
    "                  for file in range(100)])\n",
    "#paths += \",\" + \",\".join([f\"{fpath}AE/wiki_{file:0>2}\" for file in range(18)])\n",
    "\n",
    "rdd = sc.textFile(paths)\n",
    "rdd.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The corpus is cleaned by removing the article tags, any punctuation or numbers, before we split into tokenized sentences. Exactly how best to produce the data is dependent on the task at hand; we will leave in stop words for now. Details of this will be discussed in the report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_func(x):\n",
    "    try:\n",
    "        return x[0]!=\"<\"\n",
    "    except:\n",
    "        return False\n",
    "    \n",
    "table = str.maketrans('', '', string.punctuation)\n",
    "\n",
    "tokenize = lambda x: [w.lower().translate(table) for w in word_tokenize(x) if w.isalpha()]\n",
    "    \n",
    "corpus = rdd.filter(filter_func)\\\n",
    "            .flatMap(lambda x: x.split(\" . \"))\\\n",
    "            .map(tokenize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SparkML Word2Vec\n",
    "\n",
    "In order to use SparkML's built-in Word2Vec SGNS implementation we must have our data in DataFrame form. We can then fit the model: we use embedded vectors of size 300 and a window size of 2 either side of the target word. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = corpus.map(Row).toDF([\"sentence\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t0 = dt.nowutc()\n",
    "word2Vec = Word2Vec(vectorSize=300, windowSize=2, seed=42, \n",
    "                    inputCol=\"sentence\", outputCol=\"model\")\n",
    "\n",
    "model = word2Vec.fit(doc)\n",
    "t1 = dt.nowutc()\n",
    "#model.save(\"gs://fin-bucket/nwe/w2v_1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computation time for fitting:\n",
      "2302.4s\n"
     ]
    }
   ],
   "source": [
    "print(f\"Computation time for fitting:\\n{t1-t0:>7.1f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the corpus \n",
    "\n",
    "We will prepare the corpus by extracting the word-context pairs, $D$, fitering out infrequent terms and coding as integers. The coded pairs are written to CSV for the TensorFlow models to read."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collect infrequent words onto the driver node as it's a relatively small vector\n",
    "infrequent = corpus.flatMap(lambda x: [(e, 1) for e in x])\\\n",
    "                   .reduceByKey(add)\\\n",
    "                   .filter(lambda x: x[1] < 20)\\\n",
    "                   .map(lambda x: x[0]).collect()\n",
    "infrequent = set(infrequent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of words filtered out: 241431\n"
     ]
    }
   ],
   "source": [
    "print(f\"number of words filtered out:\", len(infrequent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_func(x):\n",
    "    return not any(i in infrequent for i in x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ngram2pairs(x):\n",
    "    x = list(x)\n",
    "    w = x.pop(2)\n",
    "    return [(w, c) for c in x]\n",
    "\n",
    "# prepare the word-context pairs vocabulary D\n",
    "D = corpus.flatMap(lambda x: list(ngrams(x, 5)))\\\n",
    "          .filter(filter_func)\\\n",
    "          .flatMap(ngram2pairs)\n",
    "          \n",
    "# collect the distinct vocabulary so we can swap words with numbers\n",
    "w2i = D.map(lambda x: x[1]).distinct()\\\n",
    "       .collect()\n",
    "\n",
    "D = D.map(lambda x: (w2i.index(x[0]), w2i.index(x[1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocabulary size: 31986\n"
     ]
    }
   ],
   "source": [
    "print(\"vocabulary size:\", len(w2i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This gives us a vocabulary size of 31,986. Let's write it to .txt so we can read it in again later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "D.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D.map(lambda x: \",\".join([str(e) for e in x]))\\\n",
    " .saveAsTextFile(\"hdfs:///SGNS/ds_1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# also write w2i converter to file \n",
    "fs = gcsfs.GCSFileSystem(project=\"ST446-project-1\", token=\"cloud\")\n",
    "with fs.open(\"gs://fin-bucket/SGNS/w2i_1.txt\", \"w\") as f:\n",
    "    for l in w2i:\n",
    "        f.write(f\"{l}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Compute aggregated counts\n",
    "\n",
    "The matrix factorisation approaches need only aggegrated data. Here we compute the PMI and write it to CSV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the data back from HDFS, one file for each of the 100 partitions \n",
    "paths = \",\".join([f\"hdfs://SGNS/ds_1/part-{p:0>5}\" for p in range(100)])\n",
    "D = sc.textFile(paths)\\\n",
    "      .map(lambda x: [int(i) for i in x.split(\",\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the counts. May be faster as RDD ops but DataFrames are probably\n",
    "# easier to work with here. \n",
    "df = D.toDF([\"w\", \"c\"])\\\n",
    "      .groupby(\"w\", \"c\").count().withColumnRenamed(\"count\", \"_wc\")\\\n",
    "      .withColumn('_w', func.sum('_wc').over(Window.partitionBy('w')))\\\n",
    "      .withColumn('_c', func.sum('_wc').over(Window.partitionBy('c')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the total number of word-context pairs\n",
    "t0 = dt.utcnow()\n",
    "N = df.rdd.map(lambda x: (1, x[0])).reduceByKey(add).collect()[0][1]\n",
    "t1 = dt.utcnow()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of word-context pairs: 136957326034\n"
     ]
    }
   ],
   "source": [
    "print(\"number of word-context pairs:\", N)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is quite large, no wonder it's so slow. Let's organise it into blocks and write it to HDFS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a partion function that will partition our data into blocks for the \n",
    "# blockwise computation\n",
    "def partitionFunc(split_size):\n",
    "    def split(x):\n",
    "        i = x[0] // split_size\n",
    "        j = x[1] // split_size\n",
    "\n",
    "        i = i if not i==8 else 7\n",
    "        j = j if not j==8 else 7\n",
    "    \n",
    "        return i + j*8\n",
    "    return split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It would be nice to use TensorFlow's tfrecord format but I'm struggling to get the connector to work and there shouldn't be a major compuational difference here anyway. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the word-context pair counts to tfrecord so we can read them into \n",
    "# TensorFlow. This operation is fairly expensive due to the full shuffle from \n",
    "# the repartition.\n",
    "\n",
    "# NB: can't get spark-tensorflow-connector to work properly so revert to using CSV\n",
    "\n",
    "t2 = dt.utcnow()\n",
    "df.select(\"w\", \"c\", \"_wc\")\\\n",
    "  .rdd.map(lambda x: (x[:2], x[2]))\\\n",
    "  .repartitionAndSortWithinPartitions(numPartitions=64, \n",
    "                                      partitionFunc=partitionFunc(Nij//8), \n",
    "                                      ascending=True)\\\n",
    "  .map(lambda x: (*x[0], x[1]))\\\n",
    "  .toDF([\"w\", \"c\", \"_wc\"])\\\n",
    "  .write.format(\"csv\").save(\"hdfs:///SGNS/agg_1/entries\")\n",
    "#  .write.format(\"tfrecords\").save(\"hdfs:///SGNS/agg2/entries\")\n",
    "t3 = dt.utcnow()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write vectors of word counts sorted in ascending order. Call to distinct()\n",
    "# is fairly expensive as it requires a full shuffle. \n",
    "t4 = dt.utcnow()\n",
    "df.select(\"w\", \"_w\")\\\n",
    "  .rdd\\\n",
    "  .distinct().coalesce(1)\\\n",
    "  .sortByKey()\\\n",
    "  .map(lambda x: x[1])\\\n",
    "  .saveAsTextFile(\"hdfs:///SGNS/agg_1/Ni\")\n",
    "t5 = dt.utcnow()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write vectors of context counts. \n",
    "t6 = utcnow()\n",
    "df.select(\"c\", \"_c\")\\\n",
    "  .rdd\\\n",
    "  .distinct().coalesce(1)\\\n",
    "  .sortByKey()\\\n",
    "  .map(lambda x: x[1])\\\n",
    "  .saveAsTextFile(\"hdfs:///SGNS/agg_1/Nj\")\n",
    "t7 = dt.utcnow()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computation times for computing and writing aggregate counts:\n",
      "--------------------------------------------------------------\n",
      "  691.4s: computing N\n",
      "15245.4s: writing ij pairs to HDFS\n",
      "  218.3s: writing Ni to HDFS\n",
      "  197.8s: writing Nj to HDFS\n",
      "--------------------------------------------------------------\n",
      "16352.0s  TOTAL\n"
     ]
    }
   ],
   "source": [
    "msg = \"Computation times for computing and writing aggregate counts:\\n\"+\"-\"*62\n",
    "msg += f\"\\n{t1-t0:>7.1f}s: computing N\"\n",
    "msg += f\"\\n{t3-t2:>7.1f}s: writing ij pairs to HDFS\"\n",
    "msg += f\"\\n{t5-t4:>7.1f}s: writing Ni to HDFS\"\n",
    "msg += f\"\\n{t7-t6:>7.1f}s: writing Nj to HDFS\\n\"+\"-\"*62\n",
    "msg += f\"\\n{t1+t3+t5+t7-(t0+t2+t4+t6):>7.1f}s  TOTAL\"\n",
    "print(msg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# Explicit Matrix factorisation\n",
    "\n",
    "Here we will use SparkMLlib to compute the explicit matrix factorisation solution to SGNS. First let's calculate the ppmi matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31986x31986 matrix with 8726360 entries and sparsity 0.008529297440199337\n"
     ]
    }
   ],
   "source": [
    "t8 = dt.utcnow()\n",
    "df = df.withColumn(\"ppmi\", func.log2(Nij * func.col(\"_wc\") \\\n",
    "                           / (func.col(\"_w\") * func.col(\"_c\"))))\n",
    "\n",
    "# write to hdfs\n",
    "#df.write.format(\"csv\").save(\"hdfs:///SGNS/pmi_1\")\n",
    "t9 = dt.utcnow()\n",
    "\n",
    "ncols = df.select(\"c\").distinct().count()\n",
    "nrows = ncols #df.select(\"w\").distinct().count() nrows and ncols are equal\n",
    "nentries = df.count()\n",
    "print(f\"{nrows}x{ncols} matrix with {nentries}\" + \n",
    "      f\" entries and sparsity {nentries/(ncols*nrows)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get matrix rows either from file or from the df if it exists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shiftAndPositive = func.udf(lambda x: max(x-np.log(5), 0), LongType())\n",
    "\n",
    "df = df.withColumn(\"sppmi\", shiftAndPositive(\"ppmi\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mat_rows = df.select(\"w\", \"c\", \"sppmi\").rdd.map(tuple)\n",
    "mat_rows = sc.textFile(\"hdfs:///SGNS/pmi_1/*.csv\")\\\n",
    "             .map(lambda line: line.split(\",\"))\\\n",
    "             .map(lambda x: (x[0], x[1], x[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t10 = dt.utcnow()\n",
    "mat = CoordinateMatrix(mat_rows).toRowMatrix()\n",
    "t11 = dt.utcnow()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = 300\n",
    "\n",
    "t12 = dt.utcnow()\n",
    "svd = mat.computeSVD(d, computeU=True)\n",
    "\n",
    "# compute sqrt(sigma) on the driver node\n",
    "s_root = list(map(np.sqrt, svd.s))\n",
    "\n",
    "# compute W as U dot sqrt(sigma)\n",
    "W = svd.U.rows.map(lambda x: x * s_root)\n",
    "vectors = W.collect()\n",
    "t13 = dt.utcnow()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computation times for explicit matrix factorisation:\n",
      "------------------------------------------------------------\n",
      "  935.9s: calculate PPMI and write to file\n",
      " 2485.0s: convert to row matrix\n",
      " 1307.9s: compute W from matrix factorisation\n",
      "------------------------------------------------------------\n",
      " 4728.8s TOTAL\n"
     ]
    }
   ],
   "source": [
    "msg = \"Computation times for explicit matrix factorisation:\\n\"+\"-\"*60\n",
    "msg += f\"\\n{t9-t8:>7.1f}s: calculate PPMI and write to file\"\n",
    "msg += f\"\\n{t11-t10:>7.1f}s: convert to row matrix\"\n",
    "msg += f\"\\n{t13-t12:>7.1f}s: compute W from matrix factorisation\\n\"+\"-\"*60\n",
    "msg += f\"\\n{t9+t11+t13-(t8+t10+t12):>7.1f}s TOTAL\"\n",
    "print(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs = gcsfs.GCSFileSystem(project=\"ST446-project-1\", token=\"cloud\")\n",
    "with fs.open(\"gs://fin-bucket/SGNS/Wexp_1.txt\", \"w\") as f:\n",
    "    for l in W:\n",
    "        f.write(\",\".join([str(i) for i in l])+\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# Implicit Matrix Factorisation\n",
    "\n",
    "#### In this section is a TensorFlow implementation of the implicit matrix factorisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow\n",
      "  Downloading tensorflow-2.2.0-cp37-cp37m-manylinux2010_x86_64.whl (516.2 MB)\n",
      "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 516.2 MB 2.4 kB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting gast==0.3.3\n",
      "  Downloading gast-0.3.3-py2.py3-none-any.whl (9.7 kB)\n",
      "Collecting opt-einsum>=2.3.2\n",
      "  Downloading opt_einsum-3.2.1-py3-none-any.whl (63 kB)\n",
      "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 63 kB 2.2 MB/s  eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: protobuf>=3.8.0 in /opt/conda/anaconda/lib/python3.7/site-packages (from tensorflow) (3.11.3)\n",
      "Collecting termcolor>=1.1.0\n",
      "  Downloading termcolor-1.1.0.tar.gz (3.9 kB)\n",
      "Requirement already satisfied: six>=1.12.0 in /opt/conda/anaconda/lib/python3.7/site-packages (from tensorflow) (1.12.0)\n",
      "Requirement already satisfied: scipy==1.4.1; python_version >= \"3\" in /opt/conda/anaconda/lib/python3.7/site-packages (from tensorflow) (1.4.1)\n",
      "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /opt/conda/anaconda/lib/python3.7/site-packages (from tensorflow) (0.33.6)\n",
      "Collecting google-pasta>=0.1.8\n",
      "  Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 57 kB 7.4 MB/s  eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy<2.0,>=1.16.0 in /opt/conda/anaconda/lib/python3.7/site-packages (from tensorflow) (1.18.4)\n",
      "Collecting h5py<2.11.0,>=2.10.0\n",
      "  Downloading h5py-2.10.0-cp37-cp37m-manylinux1_x86_64.whl (2.9 MB)\n",
      "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2.9 MB 46.9 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tensorboard<2.3.0,>=2.2.0\n",
      "  Downloading tensorboard-2.2.1-py3-none-any.whl (3.0 MB)\n",
      "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3.0 MB 46.6 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting astunparse==1.6.3\n",
      "  Downloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Requirement already satisfied: wrapt>=1.11.1 in /opt/conda/anaconda/lib/python3.7/site-packages (from tensorflow) (1.11.2)\n",
      "Collecting tensorflow-estimator<2.3.0,>=2.2.0\n",
      "  Downloading tensorflow_estimator-2.2.0-py2.py3-none-any.whl (454 kB)\n",
      "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 454 kB 52.0 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: grpcio>=1.8.6 in /opt/conda/anaconda/lib/python3.7/site-packages (from tensorflow) (1.28.1)\n",
      "Collecting absl-py>=0.7.0\n",
      "  Downloading absl-py-0.9.0.tar.gz (104 kB)\n",
      "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 104 kB 39.1 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting keras-preprocessing>=1.1.0\n",
      "  Downloading Keras_Preprocessing-1.1.0-py2.py3-none-any.whl (41 kB)\n",
      "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 41 kB 1.1 MB/s  eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: setuptools in /opt/conda/anaconda/lib/python3.7/site-packages (from protobuf>=3.8.0->tensorflow) (41.4.0)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /opt/conda/anaconda/lib/python3.7/site-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow) (0.16.0)\n",
      "Collecting tensorboard-plugin-wit>=1.6.0\n",
      "  Downloading tensorboard_plugin_wit-1.6.0.post3-py3-none-any.whl (777 kB)\n",
      "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 777 kB 62.6 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: google-auth<2,>=1.6.3 in /opt/conda/anaconda/lib/python3.7/site-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow) (1.13.1)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /opt/conda/anaconda/lib/python3.7/site-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow) (0.4.1)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /opt/conda/anaconda/lib/python3.7/site-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow) (2.22.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /opt/conda/anaconda/lib/python3.7/site-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow) (3.2.1)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /opt/conda/anaconda/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow) (4.1.0)\n",
      "Requirement already satisfied: rsa<4.1,>=3.1.4 in /opt/conda/anaconda/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow) (4.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/anaconda/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow) (0.2.8)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/anaconda/lib/python3.7/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.3.0,>=2.2.0->tensorflow) (1.3.0)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/anaconda/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow) (1.24.2)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /opt/conda/anaconda/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow) (2.8)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /opt/conda/anaconda/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/anaconda/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow) (2019.9.11)\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in /opt/conda/anaconda/lib/python3.7/site-packages (from rsa<4.1,>=3.1.4->google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /opt/conda/anaconda/lib/python3.7/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.3.0,>=2.2.0->tensorflow) (3.1.0)\n",
      "Building wheels for collected packages: termcolor, absl-py\n",
      "  Building wheel for termcolor (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for termcolor: filename=termcolor-1.1.0-py3-none-any.whl size=4830 sha256=5310e329f84d751b498f826efc9c7b76fa7960299b26ff6954ed4986b3fa5386\n",
      "  Stored in directory: /root/.cache/pip/wheels/3f/e3/ec/8a8336ff196023622fbcb36de0c5a5c218cbb24111d1d4c7f2\n",
      "  Building wheel for absl-py (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for absl-py: filename=absl_py-0.9.0-py3-none-any.whl size=121931 sha256=ddc03cf298570f5de9edbb914ef1d7b9626b31ed197e057482a7dcdabfb01d89\n",
      "  Stored in directory: /root/.cache/pip/wheels/cc/af/1a/498a24d0730ef484019e007bb9e8cef3ac00311a672c049a3e\n",
      "Successfully built termcolor absl-py\n",
      "Installing collected packages: gast, opt-einsum, termcolor, google-pasta, h5py, tensorboard-plugin-wit, absl-py, tensorboard, astunparse, tensorflow-estimator, keras-preprocessing, tensorflow\n",
      "  Attempting uninstall: h5py\n",
      "    Found existing installation: h5py 2.9.0\n",
      "    Uninstalling h5py-2.9.0:\n",
      "      Successfully uninstalled h5py-2.9.0\n",
      "Successfully installed absl-py-0.9.0 astunparse-1.6.3 gast-0.3.3 google-pasta-0.2.0 h5py-2.10.0 keras-preprocessing-1.1.0 opt-einsum-3.2.1 tensorboard-2.2.1 tensorboard-plugin-wit-1.6.0.post3 tensorflow-2.2.0 tensorflow-estimator-2.2.0 termcolor-1.1.0\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will define the matrix factorisation model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# variable initialiser\n",
    "nwi = lambda v, e: tf.random.normal((v,e), 0.0, 1.0/e)\n",
    "\n",
    "# It would be nice to use a distributed training strategy but it's not easy to distribute\n",
    "# the dataset. Details are discussed in the report. \n",
    "\n",
    "class MFEmbedder(Model):\n",
    "    \n",
    "    # Implicit Matrix Factorisation model for SGNS word embeddings.\n",
    "    \n",
    "    # variable initialiser\n",
    "    nwi = lambda v, e: tf.random.normal((v,e), 0.0, 1.0/e)\n",
    "    \n",
    "    def __init__(self, vocabsize, d, blocks=1):\n",
    "        super(MFEmbedder, self).__init__()\n",
    "        \n",
    "        self.vocabsize = vocabsize\n",
    "        self.d = d        \n",
    "        self.n = blocks\n",
    "        self.blklen = self._get_split_sizes()\n",
    "        \n",
    "        # initialise the trainable variables \n",
    "        self.W = tf.Variable(nwi(vocabsize, d), name=\"words\")\n",
    "        self.C = tf.Variable(nwi(vocabsize, d), name=\"conts\")\n",
    "   \n",
    "    def call(self, block):     \n",
    "        \n",
    "        # get the slice indices for the given block\n",
    "        w1, w2, c1, c2 = tf.py_function(self._get_input_for_block,\n",
    "                                        [block], [tf.int32]*4)\n",
    "        \n",
    "        # get the embedding slices\n",
    "        Wb = tf.slice(self.W, w1, w2, name=\"Wb\")\n",
    "        Cb = tf.slice(self.C, c1, c2, name=\"Cb\")\n",
    "      \n",
    "        # return their product\n",
    "        return tf.matmul(Wb, Cb, transpose_b=True)    \n",
    "    \n",
    "    def _get_split_sizes(self):\n",
    "        # calculate split sizes for a model with b blocks\n",
    "        sizes = [int(self.vocabsize // self.n)] * int(self.n)\n",
    "        # add any spill over to the last block\n",
    "        sizes[-1] += self.vocabsize % self.n\n",
    "        \n",
    "        return sizes\n",
    "\n",
    "    def _get_input_for_block(self, block):\n",
    "        \n",
    "        # compute the indices for a given block in the embedded matrices\n",
    "        \n",
    "        i, j = block // self.n, block % self.n\n",
    "        \n",
    "        ibegin = np.array([sum(self.blklen[:i]) if i!=0 else 0, 0])\n",
    "        isize = np.array([self.blklen[i], self.d])\n",
    "        \n",
    "        jbegin = np.array([sum(self.blklen[:j]) if j!=0 else 0, 0])\n",
    "        jsize = np.array([self.blklen[j], self.d])\n",
    "        \n",
    "        return [ibegin, isize, jbegin, jsize]\n",
    "\n",
    "    def _get_block_from_index(self, w, c):\n",
    "        \n",
    "        # get block number from word, context\n",
    "        \n",
    "        ss = self._get_split_sizes()\n",
    "        ss.reverse()\n",
    "        i = -1\n",
    "        j = -1\n",
    "        for k in range(self.n):\n",
    "            s = ss.pop()\n",
    "            if w <= s and i==-1:\n",
    "                i = k\n",
    "            if c <= s and j==-1:\n",
    "                j = k\n",
    "            w -= s\n",
    "            c -= s\n",
    "        return self.n * i + j\n",
    "    \n",
    "\n",
    "def SGNSLoss(Nwc, Nw, Nc, N, k):\n",
    "    # define the logistic sigmoid negative sampled loss for SGNS\n",
    "    def loss(M_hat):\n",
    "        pos_samples = tf.reduce_sum(Nwc * tf.math.log_sigmoid(M_hat))\n",
    "        neg_samples = (k / N) * \\\n",
    "                      tf.reshape(Nw, (1, -1)) @ \\\n",
    "                      tf.math.log_sigmoid(-M_hat) @ \\\n",
    "                      tf.reshape(Nc, (-1, 1))\n",
    "        return -(pos_samples + neg_samples) / N\n",
    "    return loss\n",
    "\n",
    "\n",
    "\n",
    "def formatNwc(raw, wwidth, cwidth):\n",
    "    \n",
    "    # Reformat sequence of (w, c, w-c) tuples as a matrix. It would be nice to be able to\n",
    "    # factor in sparsity but for now it returns a dense matrix. \n",
    "    \n",
    "    w, c, _wc = np.split(raw, 3, axis=1)\n",
    "              \n",
    "    Nwc = np.histogram2d(w.reshape(-1), c.reshape(-1), \n",
    "                         bins=[wwidth, cwidth], weights=_wc.reshape(-1))[0].astype(\"float32\")\n",
    "    \n",
    "    return Nwc\n",
    "\n",
    "\n",
    "\n",
    "def make_train_step(_W, _C, N, k):\n",
    "    \n",
    "    # Make the training step from the marginalised counts (_W and _C), the total \n",
    "    # number of counts (N), and k. Returns training step and loss functions. \n",
    "    \n",
    "    optimizer = tf.keras.optimizers.Adam(lr=0.1)\n",
    "    train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "     \n",
    "    @tf.function\n",
    "    def train_step(scope_model, block, Nw, Nc, Nwc):\n",
    "        \n",
    "        with tf.GradientTape() as tape:\n",
    "            # calculate W dot C and compute loss against entries in block\n",
    "            M_hat = scope_model(block) \n",
    "            sgns_loss = SGNSLoss(Nwc, Nw, Nc, N, k)\n",
    "            loss = tf.reduce_sum(sgns_loss(M_hat))\n",
    "      \n",
    "        gradients = tape.gradient(loss, scope_model.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(gradients, scope_model.trainable_variables))\n",
    "        train_loss(loss)\n",
    "\n",
    "    return train_loss, train_step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's load the entries and marginalised counts back from HDFS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, 1, 12], [0, 2, 191]]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load entries, i.e. (w, c, w-c count) triplets\n",
    "entries = sc.textFile(\"hdfs://SGNS/agg_1/entries/*\")\\\n",
    "            .map(lambda x: [int(i) for i in x[1:-1].split(\", \")])\n",
    "entries.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31986"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the marginalised counts and store on driver, this is the vector\n",
    "# of counts ordered by word index\n",
    "_W = sc.textFile(\"hdfs://SGNS/agg_1/Ni/*\")\\\n",
    "       .map(float)\\\n",
    "       .collect()\n",
    "\n",
    "_C = sc.textFile(\"hdfs://SGNS/agg_1/Nj/*\")\\\n",
    "       .map(float)\\\n",
    "       .collect()\n",
    "\n",
    "# Calculate N as the sum of all counts and check we have right vocab size\n",
    "N = sum(_W)\n",
    "print(len(_W))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n",
      " block 0: loss 0.0262 (0:00:11.321680 seconds)\n",
      " block 8: loss 0.0253 (0:00:11.431378 seconds)\n",
      " block 16: loss 0.0266 (0:00:11.100778 seconds)\n",
      " block 24: loss 0.0307 (0:00:11.541203 seconds)\n",
      " block 32: loss 0.0355 (0:00:11.161244 seconds)\n",
      " block 40: loss 0.0329 (0:00:11.174043 seconds)\n",
      " block 48: loss 0.0317 (0:00:11.147855 seconds)\n",
      " block 56: loss 0.0300 (0:00:11.324976 seconds)\n",
      " block 1: loss 0.0296 (0:00:11.118824 seconds)\n",
      " block 9: loss 0.0290 (0:00:11.201331 seconds)\n",
      " block 17: loss 0.0289 (0:00:11.243436 seconds)\n",
      " block 25: loss 0.0300 (0:00:11.268842 seconds)\n",
      " block 33: loss 0.0320 (0:00:11.273990 seconds)\n",
      " block 41: loss 0.0311 (0:00:11.453311 seconds)\n",
      " block 49: loss 0.0307 (0:00:11.370787 seconds)\n",
      " block 57: loss 0.0300 (0:00:11.414748 seconds)\n",
      " block 2: loss 0.0300 (0:00:11.221296 seconds)\n",
      " block 10: loss 0.0299 (0:00:11.316856 seconds)\n",
      " block 26: loss 0.0308 (0:00:11.399921 seconds)\n",
      " block 34: loss 0.0325 (0:00:11.435838 seconds)\n",
      " block 42: loss 0.0321 (0:00:11.382403 seconds)\n",
      " block 50: loss 0.0319 (0:00:11.418272 seconds)\n",
      " block 18: loss 0.0319 (0:00:11.417508 seconds)\n",
      " block 58: loss 0.0315 (0:00:11.826645 seconds)\n",
      " block 3: loss 0.0323 (0:00:11.742105 seconds)\n",
      " block 11: loss 0.0326 (0:00:11.370300 seconds)\n",
      " block 19: loss 0.0332 (0:00:11.420811 seconds)\n",
      " block 27: loss 0.0345 (0:00:11.344127 seconds)\n",
      " block 35: loss 0.0365 (0:00:11.618212 seconds)\n",
      " block 51: loss 0.0367 (0:00:11.408032 seconds)\n",
      " block 43: loss 0.0366 (0:00:11.332673 seconds)\n",
      " block 59: loss 0.0365 (0:00:11.629234 seconds)\n",
      " block 20: loss 0.0382 (0:00:11.360452 seconds)\n",
      " block 28: loss 0.0398 (0:00:11.426881 seconds)\n",
      " block 4: loss 0.0402 (0:00:11.520866 seconds)\n",
      " block 12: loss 0.0407 (0:00:11.522689 seconds)\n",
      " block 36: loss 0.0423 (0:00:11.592935 seconds)\n",
      " block 44: loss 0.0424 (0:00:11.419390 seconds)\n",
      " block 5: loss 0.0419 (0:00:11.326088 seconds)\n",
      " block 60: loss 0.0419 (0:00:11.681960 seconds)\n",
      " block 21: loss 0.0415 (0:00:11.435847 seconds)\n",
      " block 29: loss 0.0414 (0:00:11.513539 seconds)\n",
      " block 13: loss 0.0409 (0:00:11.499099 seconds)\n",
      " block 37: loss 0.0410 (0:00:11.487546 seconds)\n",
      " block 45: loss 0.0404 (0:00:11.518437 seconds)\n",
      " block 53: loss 0.0399 (0:00:11.468258 seconds)\n",
      " block 61: loss 0.0394 (0:00:12.280674 seconds)\n",
      " block 6: loss 0.0391 (0:00:11.395751 seconds)\n",
      " block 14: loss 0.0388 (0:00:11.397936 seconds)\n",
      " block 22: loss 0.0386 (0:00:11.824415 seconds)\n",
      " block 52: loss 0.0390 (0:00:11.803405 seconds)\n",
      " block 30: loss 0.0390 (0:00:11.780150 seconds)\n",
      " block 38: loss 0.0394 (0:00:11.577342 seconds)\n",
      " block 46: loss 0.0391 (0:00:11.383018 seconds)\n",
      " block 54: loss 0.0388 (0:00:11.513304 seconds)\n",
      " block 62: loss 0.0385 (0:00:11.797849 seconds)\n",
      " block 7: loss 0.0381 (0:00:11.654002 seconds)\n",
      " block 15: loss 0.0378 (0:00:11.557333 seconds)\n",
      " block 23: loss 0.0375 (0:00:11.333574 seconds)\n",
      " block 31: loss 0.0375 (0:00:11.284632 seconds)\n",
      " block 39: loss 0.0376 (0:00:11.388799 seconds)\n",
      " block 47: loss 0.0372 (0:00:11.212881 seconds)\n",
      " block 55: loss 0.0369 (0:00:11.400218 seconds)\n",
      " block 63: loss 0.0366 (0:00:11.215370 seconds)\n",
      " epoch complete! 0:12:32.868588\n",
      "\n",
      "Epoch 1\n",
      " block 0: loss 0.0364 (0:00:11.044716 seconds)\n",
      " block 8: loss 0.0362 (0:00:10.890821 seconds)\n",
      " block 16: loss 0.0360 (0:00:11.030372 seconds)\n",
      " block 24: loss 0.0361 (0:00:10.904620 seconds)\n",
      " block 32: loss 0.0364 (0:00:11.009483 seconds)\n",
      " block 40: loss 0.0361 (0:00:11.016814 seconds)\n",
      " block 48: loss 0.0359 (0:00:10.965772 seconds)\n",
      " block 56: loss 0.0357 (0:00:11.161958 seconds)\n",
      " block 1: loss 0.0355 (0:00:11.043595 seconds)\n",
      " block 9: loss 0.0354 (0:00:10.993385 seconds)\n",
      " block 17: loss 0.0353 (0:00:10.997078 seconds)\n",
      " block 25: loss 0.0354 (0:00:11.171880 seconds)\n",
      " block 33: loss 0.0356 (0:00:10.985053 seconds)\n",
      " block 41: loss 0.0354 (0:00:11.181437 seconds)\n",
      " block 49: loss 0.0353 (0:00:11.034082 seconds)\n",
      " block 57: loss 0.0351 (0:00:11.103462 seconds)\n",
      " block 2: loss 0.0350 (0:00:11.048989 seconds)\n",
      " block 10: loss 0.0349 (0:00:11.024300 seconds)\n",
      " block 26: loss 0.0351 (0:00:11.011468 seconds)\n",
      " block 34: loss 0.0354 (0:00:11.007811 seconds)\n",
      " block 42: loss 0.0353 (0:00:11.003506 seconds)\n",
      " block 50: loss 0.0352 (0:00:10.983317 seconds)\n",
      " block 18: loss 0.0351 (0:00:11.154389 seconds)\n",
      " block 58: loss 0.0350 (0:00:11.084444 seconds)\n",
      " block 3: loss 0.0351 (0:00:11.027813 seconds)\n",
      " block 11: loss 0.0352 (0:00:10.978022 seconds)\n",
      " block 19: loss 0.0353 (0:00:11.019993 seconds)\n",
      " block 27: loss 0.0357 (0:00:10.968521 seconds)\n",
      " block 35: loss 0.0364 (0:00:10.913889 seconds)\n",
      " block 51: loss 0.0364 (0:00:11.038168 seconds)\n",
      " block 43: loss 0.0364 (0:00:11.026009 seconds)\n",
      " block 59: loss 0.0364 (0:00:11.103184 seconds)\n",
      " block 20: loss 0.0367 (0:00:10.928360 seconds)\n",
      " block 28: loss 0.0373 (0:00:10.897857 seconds)\n",
      " block 4: loss 0.0374 (0:00:11.032103 seconds)\n",
      " block 12: loss 0.0376 (0:00:10.948277 seconds)\n",
      " block 36: loss 0.0383 (0:00:10.964843 seconds)\n",
      " block 44: loss 0.0384 (0:00:10.906703 seconds)\n",
      " block 5: loss 0.0382 (0:00:11.154144 seconds)\n",
      " block 60: loss 0.0382 (0:00:11.057497 seconds)\n",
      " block 21: loss 0.0381 (0:00:10.957060 seconds)\n",
      " block 29: loss 0.0380 (0:00:10.990786 seconds)\n",
      " block 13: loss 0.0379 (0:00:10.881429 seconds)\n",
      " block 37: loss 0.0379 (0:00:10.832084 seconds)\n",
      " block 45: loss 0.0377 (0:00:10.976520 seconds)\n",
      " block 53: loss 0.0376 (0:00:10.921178 seconds)\n",
      " block 61: loss 0.0374 (0:00:11.016290 seconds)\n",
      " block 6: loss 0.0372 (0:00:10.975568 seconds)\n",
      " block 14: loss 0.0371 (0:00:11.016988 seconds)\n",
      " block 22: loss 0.0371 (0:00:11.063452 seconds)\n",
      " block 52: loss 0.0373 (0:00:10.991491 seconds)\n",
      " block 30: loss 0.0373 (0:00:11.019877 seconds)\n",
      " block 38: loss 0.0375 (0:00:10.973081 seconds)\n",
      " block 46: loss 0.0373 (0:00:10.970078 seconds)\n",
      " block 54: loss 0.0372 (0:00:10.986754 seconds)\n",
      " block 62: loss 0.0371 (0:00:11.047436 seconds)\n",
      " block 7: loss 0.0369 (0:00:11.054787 seconds)\n",
      " block 15: loss 0.0368 (0:00:11.010236 seconds)\n",
      " block 23: loss 0.0367 (0:00:10.932919 seconds)\n",
      " block 31: loss 0.0366 (0:00:10.924361 seconds)\n",
      " block 39: loss 0.0367 (0:00:10.907183 seconds)\n",
      " block 47: loss 0.0365 (0:00:10.877642 seconds)\n",
      " block 55: loss 0.0364 (0:00:10.877687 seconds)\n",
      " block 63: loss 0.0362 (0:00:10.824864 seconds)\n",
      " epoch complete! 0:12:03.473642\n",
      "\n",
      "Epoch 2\n",
      " block 0: loss 0.0361 (0:00:10.897416 seconds)\n",
      " block 8: loss 0.0360 (0:00:10.885235 seconds)\n",
      " block 16: loss 0.0360 (0:00:10.928435 seconds)\n",
      " block 24: loss 0.0360 (0:00:10.992347 seconds)\n",
      " block 32: loss 0.0361 (0:00:10.966470 seconds)\n",
      " block 40: loss 0.0360 (0:00:10.798382 seconds)\n",
      " block 48: loss 0.0359 (0:00:10.802730 seconds)\n",
      " block 56: loss 0.0358 (0:00:11.023568 seconds)\n",
      " block 1: loss 0.0357 (0:00:10.787810 seconds)\n",
      " block 9: loss 0.0356 (0:00:10.799522 seconds)\n",
      " block 17: loss 0.0355 (0:00:10.964957 seconds)\n",
      " block 25: loss 0.0356 (0:00:10.828092 seconds)\n",
      " block 33: loss 0.0357 (0:00:10.793402 seconds)\n",
      " block 41: loss 0.0356 (0:00:10.834758 seconds)\n",
      " block 49: loss 0.0355 (0:00:10.800185 seconds)\n",
      " block 57: loss 0.0354 (0:00:10.863648 seconds)\n",
      " block 2: loss 0.0354 (0:00:10.728336 seconds)\n",
      " block 10: loss 0.0353 (0:00:10.812577 seconds)\n",
      " block 26: loss 0.0354 (0:00:10.756710 seconds)\n",
      " block 34: loss 0.0356 (0:00:10.769838 seconds)\n",
      " block 42: loss 0.0355 (0:00:10.743217 seconds)\n",
      " block 50: loss 0.0355 (0:00:10.746495 seconds)\n",
      " block 18: loss 0.0354 (0:00:10.767845 seconds)\n",
      " block 58: loss 0.0354 (0:00:10.880598 seconds)\n",
      " block 3: loss 0.0354 (0:00:10.799068 seconds)\n",
      " block 11: loss 0.0354 (0:00:10.794740 seconds)\n",
      " block 19: loss 0.0355 (0:00:10.827615 seconds)\n",
      " block 27: loss 0.0358 (0:00:10.741636 seconds)\n",
      " block 35: loss 0.0362 (0:00:10.783049 seconds)\n",
      " block 51: loss 0.0362 (0:00:10.870053 seconds)\n",
      " block 43: loss 0.0362 (0:00:10.944468 seconds)\n",
      " block 59: loss 0.0362 (0:00:10.914008 seconds)\n",
      " block 20: loss 0.0365 (0:00:10.920444 seconds)\n",
      " block 28: loss 0.0369 (0:00:10.752372 seconds)\n",
      " block 4: loss 0.0370 (0:00:10.738847 seconds)\n",
      " block 12: loss 0.0371 (0:00:10.734624 seconds)\n",
      " block 36: loss 0.0375 (0:00:10.747757 seconds)\n",
      " block 44: loss 0.0375 (0:00:10.792263 seconds)\n",
      " block 5: loss 0.0374 (0:00:10.759712 seconds)\n",
      " block 60: loss 0.0375 (0:00:10.966446 seconds)\n",
      " block 21: loss 0.0374 (0:00:10.722841 seconds)\n",
      " block 29: loss 0.0374 (0:00:10.748527 seconds)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " block 13: loss 0.0373 (0:00:10.739888 seconds)\n",
      " block 37: loss 0.0373 (0:00:10.780121 seconds)\n",
      " block 45: loss 0.0372 (0:00:10.766878 seconds)\n",
      " block 53: loss 0.0371 (0:00:10.754018 seconds)\n",
      " block 61: loss 0.0370 (0:00:10.959182 seconds)\n",
      " block 6: loss 0.0369 (0:00:10.729724 seconds)\n",
      " block 14: loss 0.0368 (0:00:10.767467 seconds)\n",
      " block 22: loss 0.0368 (0:00:10.703345 seconds)\n",
      " block 52: loss 0.0369 (0:00:10.727925 seconds)\n",
      " block 30: loss 0.0369 (0:00:10.708441 seconds)\n",
      " block 38: loss 0.0370 (0:00:10.744270 seconds)\n",
      " block 46: loss 0.0369 (0:00:10.845920 seconds)\n",
      " block 54: loss 0.0369 (0:00:10.737859 seconds)\n",
      " block 62: loss 0.0368 (0:00:10.851467 seconds)\n",
      " block 7: loss 0.0367 (0:00:10.777085 seconds)\n",
      " block 15: loss 0.0366 (0:00:10.913226 seconds)\n",
      " block 23: loss 0.0365 (0:00:10.726767 seconds)\n",
      " block 31: loss 0.0365 (0:00:10.802244 seconds)\n",
      " block 39: loss 0.0365 (0:00:11.084384 seconds)\n",
      " block 39: loss 0.0364 (0:00:11.007223 seconds)\n",
      " block 55: loss 0.0363 (0:00:10.810536 seconds)\n",
      " block 63: loss 0.0362 (0:00:10.785801 seconds)\n",
      " epoch complete! 0:11:52.076744\n",
      "\n",
      "Epoch 3\n",
      " block 0: loss 0.0361 (0:00:10.903792 seconds)\n",
      " block 8: loss 0.0361 (0:00:10.832671 seconds)\n",
      " block 16: loss 0.0360 (0:00:10.751580 seconds)\n",
      " block 24: loss 0.0361 (0:00:10.918275 seconds)\n",
      " block 32: loss 0.0361 (0:00:10.774422 seconds)\n",
      " block 40: loss 0.0361 (0:00:10.829352 seconds)\n",
      " block 48: loss 0.0360 (0:00:10.818633 seconds)\n",
      " block 56: loss 0.0359 (0:00:10.913837 seconds)\n",
      " block 1: loss 0.0358 (0:00:10.760242 seconds)\n",
      " block 9: loss 0.0358 (0:00:10.807746 seconds)\n",
      " block 17: loss 0.0357 (0:00:10.816563 seconds)\n",
      " block 25: loss 0.0358 (0:00:11.333932 seconds)\n",
      " block 33: loss 0.0359 (0:00:10.781648 seconds)\n",
      " block 41: loss 0.0358 (0:00:10.795430 seconds)\n",
      " block 49: loss 0.0357 (0:00:10.834998 seconds)\n",
      " block 57: loss 0.0357 (0:00:10.991517 seconds)\n",
      " block 2: loss 0.0356 (0:00:10.758059 seconds)\n",
      " block 10: loss 0.0356 (0:00:10.774592 seconds)\n",
      " block 26: loss 0.0356 (0:00:10.804612 seconds)\n",
      " block 34: loss 0.0358 (0:00:10.886248 seconds)\n",
      " block 42: loss 0.0357 (0:00:10.916933 seconds)\n",
      " block 50: loss 0.0357 (0:00:11.141673 seconds)\n",
      " block 18: loss 0.0357 (0:00:10.881498 seconds)\n",
      " block 58: loss 0.0356 (0:00:10.920168 seconds)\n",
      " block 3: loss 0.0356 (0:00:11.039065 seconds)\n",
      " block 11: loss 0.0357 (0:00:10.846991 seconds)\n",
      " block 19: loss 0.0357 (0:00:10.827084 seconds)\n",
      " block 27: loss 0.0359 (0:00:10.769846 seconds)\n",
      " block 35: loss 0.0362 (0:00:10.810337 seconds)\n",
      " block 51: loss 0.0362 (0:00:10.824249 seconds)\n",
      " block 43: loss 0.0362 (0:00:10.855990 seconds)\n",
      " block 59: loss 0.0362 (0:00:10.894920 seconds)\n",
      " block 20: loss 0.0363 (0:00:10.955183 seconds)\n",
      " block 28: loss 0.0366 (0:00:10.830545 seconds)\n",
      " block 4: loss 0.0367 (0:00:11.001748 seconds)\n",
      " block 12: loss 0.0368 (0:00:10.859445 seconds)\n",
      " block 36: loss 0.0370 (0:00:10.916323 seconds)\n",
      " block 44: loss 0.0371 (0:00:10.843065 seconds)\n",
      " block 5: loss 0.0370 (0:00:10.919471 seconds)\n",
      " block 60: loss 0.0370 (0:00:10.943682 seconds)\n",
      " block 21: loss 0.0370 (0:00:10.958559 seconds)\n",
      " block 29: loss 0.0370 (0:00:10.792125 seconds)\n",
      " block 13: loss 0.0369 (0:00:10.813824 seconds)\n",
      " block 37: loss 0.0369 (0:00:10.803244 seconds)\n",
      " block 45: loss 0.0368 (0:00:10.928619 seconds)\n",
      " block 53: loss 0.0368 (0:00:10.837581 seconds)\n",
      " block 61: loss 0.0367 (0:00:10.901654 seconds)\n",
      " block 6: loss 0.0366 (0:00:10.900008 seconds)\n",
      " block 14: loss 0.0366 (0:00:11.048712 seconds)\n",
      " block 22: loss 0.0365 (0:00:10.913984 seconds)\n",
      " block 52: loss 0.0366 (0:00:10.965855 seconds)\n",
      " block 30: loss 0.0366 (0:00:11.128362 seconds)\n",
      " block 38: loss 0.0367 (0:00:11.042636 seconds)\n",
      " block 46: loss 0.0367 (0:00:10.874278 seconds)\n",
      " block 54: loss 0.0366 (0:00:10.991902 seconds)\n",
      " block 62: loss 0.0365 (0:00:10.972924 seconds)\n",
      " block 7: loss 0.0365 (0:00:10.869140 seconds)\n",
      " block 15: loss 0.0364 (0:00:11.025666 seconds)\n",
      " block 23: loss 0.0363 (0:00:10.872296 seconds)\n",
      " block 31: loss 0.0363 (0:00:11.026565 seconds)\n",
      " block 39: loss 0.0364 (0:00:10.915684 seconds)\n",
      " block 47: loss 0.0363 (0:00:11.017363 seconds)\n",
      " block 55: loss 0.0362 (0:00:10.926565 seconds)\n",
      " block 63: loss 0.0361 (0:00:10.856259 seconds)\n",
      " epoch complete! 0:11:57.184277\n",
      "\n",
      "Epoch 4\n",
      " block 0: loss 0.0361 (0:00:10.996082 seconds)\n",
      " block 8: loss 0.0360 (0:00:10.893777 seconds)\n",
      " block 16: loss 0.0360 (0:00:10.873205 seconds)\n",
      " block 24: loss 0.0360 (0:00:10.889888 seconds)\n",
      " block 32: loss 0.0361 (0:00:10.863073 seconds)\n",
      " block 40: loss 0.0360 (0:00:10.814012 seconds)\n",
      " block 48: loss 0.0360 (0:00:10.841418 seconds)\n",
      " block 56: loss 0.0359 (0:00:10.919973 seconds)\n",
      " block 1: loss 0.0359 (0:00:10.873633 seconds)\n",
      " block 9: loss 0.0358 (0:00:10.917388 seconds)\n",
      " block 17: loss 0.0358 (0:00:10.830939 seconds)\n",
      " block 25: loss 0.0358 (0:00:11.020583 seconds)\n",
      " block 33: loss 0.0359 (0:00:10.797738 seconds)\n",
      " block 41: loss 0.0358 (0:00:10.825100 seconds)\n",
      " block 49: loss 0.0358 (0:00:10.811891 seconds)\n",
      " block 57: loss 0.0357 (0:00:10.962224 seconds)\n",
      " block 2: loss 0.0357 (0:00:10.825224 seconds)\n",
      " block 10: loss 0.0357 (0:00:10.846576 seconds)\n",
      " block 26: loss 0.0357 (0:00:10.814928 seconds)\n",
      " block 34: loss 0.0358 (0:00:10.849387 seconds)\n",
      " block 42: loss 0.0358 (0:00:10.795287 seconds)\n",
      " block 50: loss 0.0357 (0:00:10.761772 seconds)\n",
      " block 18: loss 0.0357 (0:00:10.835094 seconds)\n",
      " block 58: loss 0.0357 (0:00:10.852431 seconds)\n",
      " block 3: loss 0.0357 (0:00:10.888403 seconds)\n",
      " block 11: loss 0.0357 (0:00:10.845651 seconds)\n",
      " block 19: loss 0.0357 (0:00:10.872812 seconds)\n",
      " block 27: loss 0.0359 (0:00:10.818069 seconds)\n",
      " block 35: loss 0.0361 (0:00:10.790990 seconds)\n",
      " block 51: loss 0.0361 (0:00:10.805815 seconds)\n",
      " block 43: loss 0.0361 (0:00:10.817959 seconds)\n",
      " block 59: loss 0.0361 (0:00:10.865287 seconds)\n",
      " block 20: loss 0.0363 (0:00:10.807896 seconds)\n",
      " block 28: loss 0.0365 (0:00:11.087548 seconds)\n",
      " block 4: loss 0.0365 (0:00:10.828462 seconds)\n",
      " block 12: loss 0.0366 (0:00:10.851933 seconds)\n",
      " block 36: loss 0.0368 (0:00:10.850962 seconds)\n",
      " block 44: loss 0.0368 (0:00:10.835488 seconds)\n",
      " block 5: loss 0.0368 (0:00:10.860519 seconds)\n",
      " block 60: loss 0.0368 (0:00:11.105953 seconds)\n",
      " block 21: loss 0.0368 (0:00:10.854765 seconds)\n",
      " block 29: loss 0.0367 (0:00:10.828048 seconds)\n",
      " block 13: loss 0.0367 (0:00:10.871263 seconds)\n",
      " block 37: loss 0.0367 (0:00:10.967910 seconds)\n",
      " block 45: loss 0.0366 (0:00:10.829838 seconds)\n",
      " block 53: loss 0.0366 (0:00:10.866484 seconds)\n",
      " block 61: loss 0.0365 (0:00:10.847083 seconds)\n",
      " block 6: loss 0.0365 (0:00:10.866537 seconds)\n",
      " block 14: loss 0.0364 (0:00:10.780330 seconds)\n",
      " block 22: loss 0.0364 (0:00:10.795342 seconds)\n",
      " block 52: loss 0.0365 (0:00:10.820004 seconds)\n",
      " block 30: loss 0.0365 (0:00:10.764132 seconds)\n",
      " block 38: loss 0.0366 (0:00:10.856526 seconds)\n",
      " block 46: loss 0.0365 (0:00:10.787396 seconds)\n",
      " block 54: loss 0.0365 (0:00:11.104001 seconds)\n",
      " block 62: loss 0.0364 (0:00:10.903848 seconds)\n",
      " block 7: loss 0.0364 (0:00:10.834438 seconds)\n",
      " block 15: loss 0.0363 (0:00:10.868804 seconds)\n",
      " block 23: loss 0.0363 (0:00:10.827968 seconds)\n",
      " block 31: loss 0.0363 (0:00:10.870121 seconds)\n",
      " block 39: loss 0.0363 (0:00:10.986303 seconds)\n",
      " block 47: loss 0.0362 (0:00:10.868049 seconds)\n",
      " block 55: loss 0.0362 (0:00:10.857783 seconds)\n",
      " block 63: loss 0.0361 (0:00:10.855709 seconds)\n",
      " epoch complete! 0:11:54.803964\n",
      "\n",
      "Epoch 5\n",
      " block 0: loss 0.0360 (0:00:10.829646 seconds)\n",
      " block 8: loss 0.0360 (0:00:10.817866 seconds)\n",
      " block 16: loss 0.0360 (0:00:10.994903 seconds)\n",
      " block 24: loss 0.0360 (0:00:10.898575 seconds)\n",
      " block 32: loss 0.0360 (0:00:10.879146 seconds)\n",
      " block 40: loss 0.0360 (0:00:10.885113 seconds)\n",
      " block 48: loss 0.0360 (0:00:10.859887 seconds)\n",
      " block 56: loss 0.0359 (0:00:11.082600 seconds)\n",
      " block 1: loss 0.0359 (0:00:10.768638 seconds)\n",
      " block 9: loss 0.0358 (0:00:10.820398 seconds)\n",
      " block 17: loss 0.0358 (0:00:10.826052 seconds)\n",
      " block 25: loss 0.0358 (0:00:10.839256 seconds)\n",
      " block 33: loss 0.0359 (0:00:10.935862 seconds)\n",
      " block 41: loss 0.0358 (0:00:10.862637 seconds)\n",
      " block 49: loss 0.0358 (0:00:10.831232 seconds)\n",
      " block 57: loss 0.0357 (0:00:10.849968 seconds)\n",
      " block 2: loss 0.0357 (0:00:10.796495 seconds)\n",
      " block 10: loss 0.0357 (0:00:10.852876 seconds)\n",
      " block 26: loss 0.0357 (0:00:10.902397 seconds)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " block 34: loss 0.0358 (0:00:10.852924 seconds)\n",
      " block 42: loss 0.0358 (0:00:10.892574 seconds)\n",
      " block 50: loss 0.0358 (0:00:10.821951 seconds)\n",
      " block 18: loss 0.0358 (0:00:10.855158 seconds)\n",
      " block 58: loss 0.0357 (0:00:10.896177 seconds)\n",
      " block 3: loss 0.0357 (0:00:10.828233 seconds)\n",
      " block 11: loss 0.0358 (0:00:10.834565 seconds)\n",
      " block 19: loss 0.0358 (0:00:10.801050 seconds)\n",
      " block 27: loss 0.0359 (0:00:10.850566 seconds)\n",
      " block 35: loss 0.0361 (0:00:10.803727 seconds)\n",
      " block 51: loss 0.0361 (0:00:10.854362 seconds)\n",
      " block 43: loss 0.0361 (0:00:10.978028 seconds)\n",
      " block 59: loss 0.0361 (0:00:10.960082 seconds)\n",
      " block 20: loss 0.0362 (0:00:10.850644 seconds)\n",
      " block 28: loss 0.0364 (0:00:10.904728 seconds)\n",
      " block 4: loss 0.0364 (0:00:10.920961 seconds)\n",
      " block 12: loss 0.0365 (0:00:11.163785 seconds)\n",
      " block 36: loss 0.0367 (0:00:10.836470 seconds)\n",
      " block 44: loss 0.0367 (0:00:10.850332 seconds)\n",
      " block 5: loss 0.0367 (0:00:10.794272 seconds)\n",
      " block 60: loss 0.0367 (0:00:10.874131 seconds)\n",
      " block 21: loss 0.0366 (0:00:10.815288 seconds)\n",
      " block 29: loss 0.0366 (0:00:10.845647 seconds)\n",
      " block 13: loss 0.0366 (0:00:10.851660 seconds)\n",
      " block 37: loss 0.0366 (0:00:10.820529 seconds)\n",
      " block 45: loss 0.0366 (0:00:10.815869 seconds)\n",
      " block 53: loss 0.0365 (0:00:10.826417 seconds)\n",
      " block 61: loss 0.0364 (0:00:10.839392 seconds)\n",
      " block 6: loss 0.0364 (0:00:10.865377 seconds)\n",
      " block 14: loss 0.0364 (0:00:10.792183 seconds)\n",
      " block 22: loss 0.0364 (0:00:10.799836 seconds)\n",
      " block 52: loss 0.0364 (0:00:10.771109 seconds)\n",
      " block 30: loss 0.0364 (0:00:10.853580 seconds)\n",
      " block 38: loss 0.0365 (0:00:10.798989 seconds)\n",
      " block 46: loss 0.0364 (0:00:10.809276 seconds)\n",
      " block 54: loss 0.0364 (0:00:10.824482 seconds)\n",
      " block 62: loss 0.0364 (0:00:11.078229 seconds)\n",
      " block 7: loss 0.0363 (0:00:10.876278 seconds)\n",
      " block 15: loss 0.0363 (0:00:11.112807 seconds)\n",
      " block 23: loss 0.0362 (0:00:10.891046 seconds)\n",
      " block 31: loss 0.0362 (0:00:10.927181 seconds)\n",
      " block 39: loss 0.0362 (0:00:10.873879 seconds)\n",
      " block 47: loss 0.0362 (0:00:10.862864 seconds)\n",
      " block 55: loss 0.0361 (0:00:10.888512 seconds)\n",
      " block 63: loss 0.0361 (0:00:11.036685 seconds)\n",
      " epoch complete! 0:11:55.240535\n",
      "\n",
      "Epoch 6\n",
      " block 0: loss 0.0361 (0:00:10.834340 seconds)\n",
      " block 8: loss 0.0360 (0:00:10.791529 seconds)\n",
      " block 16: loss 0.0360 (0:00:10.787802 seconds)\n",
      " block 24: loss 0.0360 (0:00:10.762020 seconds)\n",
      " block 32: loss 0.0361 (0:00:10.815342 seconds)\n",
      " block 40: loss 0.0360 (0:00:10.786795 seconds)\n",
      " block 48: loss 0.0360 (0:00:10.764128 seconds)\n",
      " block 56: loss 0.0359 (0:00:10.850777 seconds)\n",
      " block 1: loss 0.0359 (0:00:10.805655 seconds)\n",
      " block 9: loss 0.0359 (0:00:11.113008 seconds)\n",
      " block 17: loss 0.0359 (0:00:10.845219 seconds)\n",
      " block 25: loss 0.0359 (0:00:10.830434 seconds)\n",
      " block 33: loss 0.0359 (0:00:10.894606 seconds)\n",
      " block 41: loss 0.0359 (0:00:10.809469 seconds)\n",
      " block 49: loss 0.0358 (0:00:10.817851 seconds)\n",
      " block 57: loss 0.0358 (0:00:10.837929 seconds)\n",
      " block 2: loss 0.0358 (0:00:10.764490 seconds)\n",
      " block 10: loss 0.0358 (0:00:10.768288 seconds)\n",
      " block 26: loss 0.0358 (0:00:10.818389 seconds)\n",
      " block 34: loss 0.0359 (0:00:10.820291 seconds)\n",
      " block 42: loss 0.0358 (0:00:10.876381 seconds)\n",
      " block 50: loss 0.0358 (0:00:10.965999 seconds)\n",
      " block 18: loss 0.0358 (0:00:11.300146 seconds)\n",
      " block 58: loss 0.0358 (0:00:10.901575 seconds)\n",
      " block 3: loss 0.0358 (0:00:10.840822 seconds)\n",
      " block 11: loss 0.0358 (0:00:10.790668 seconds)\n",
      " block 19: loss 0.0358 (0:00:10.853607 seconds)\n",
      " block 27: loss 0.0359 (0:00:10.939006 seconds)\n",
      " block 35: loss 0.0361 (0:00:10.797055 seconds)\n",
      " block 51: loss 0.0361 (0:00:10.803867 seconds)\n",
      " block 43: loss 0.0361 (0:00:10.812753 seconds)\n",
      " block 59: loss 0.0361 (0:00:10.906590 seconds)\n",
      " block 20: loss 0.0361 (0:00:10.817667 seconds)\n",
      " block 28: loss 0.0363 (0:00:10.830620 seconds)\n",
      " block 4: loss 0.0363 (0:00:10.992617 seconds)\n",
      " block 12: loss 0.0364 (0:00:10.853042 seconds)\n",
      " block 36: loss 0.0365 (0:00:10.847970 seconds)\n",
      " block 44: loss 0.0366 (0:00:10.751794 seconds)\n",
      " block 5: loss 0.0365 (0:00:10.784517 seconds)\n",
      " block 60: loss 0.0365 (0:00:10.836314 seconds)\n",
      " block 21: loss 0.0365 (0:00:10.749708 seconds)\n",
      " block 29: loss 0.0365 (0:00:10.755117 seconds)\n",
      " block 13: loss 0.0365 (0:00:10.810354 seconds)\n",
      " block 37: loss 0.0365 (0:00:10.756657 seconds)\n",
      " block 45: loss 0.0364 (0:00:10.794780 seconds)\n",
      " block 53: loss 0.0364 (0:00:10.749557 seconds)\n",
      " block 61: loss 0.0364 (0:00:10.757721 seconds)\n",
      " block 6: loss 0.0363 (0:00:10.727145 seconds)\n",
      " block 14: loss 0.0363 (0:00:10.878421 seconds)\n",
      " block 22: loss 0.0363 (0:00:10.862309 seconds)\n",
      " block 52: loss 0.0363 (0:00:10.780290 seconds)\n",
      " block 30: loss 0.0363 (0:00:10.769569 seconds)\n",
      " block 38: loss 0.0364 (0:00:10.781201 seconds)\n",
      " block 46: loss 0.0364 (0:00:10.750289 seconds)\n",
      " block 54: loss 0.0363 (0:00:10.830904 seconds)\n",
      " block 62: loss 0.0363 (0:00:11.063131 seconds)\n",
      " block 7: loss 0.0362 (0:00:10.806271 seconds)\n",
      " block 15: loss 0.0362 (0:00:10.836119 seconds)\n",
      " block 23: loss 0.0362 (0:00:10.829996 seconds)\n",
      " block 31: loss 0.0362 (0:00:10.822168 seconds)\n",
      " block 39: loss 0.0362 (0:00:10.817869 seconds)\n",
      " block 47: loss 0.0361 (0:00:10.801905 seconds)\n",
      " block 55: loss 0.0361 (0:00:10.719247 seconds)\n",
      " block 63: loss 0.0361 (0:00:10.735008 seconds)\n",
      " epoch complete! 0:11:52.852939\n",
      "\n",
      "Epoch 7\n",
      " block 0: loss 0.0360 (0:00:10.735541 seconds)\n",
      " block 8: loss 0.0360 (0:00:10.659613 seconds)\n",
      " block 16: loss 0.0360 (0:00:10.699364 seconds)\n",
      " block 24: loss 0.0360 (0:00:10.629489 seconds)\n",
      " block 32: loss 0.0360 (0:00:10.676479 seconds)\n",
      " block 40: loss 0.0360 (0:00:10.655455 seconds)\n",
      " block 48: loss 0.0360 (0:00:10.673857 seconds)\n",
      " block 56: loss 0.0359 (0:00:10.741065 seconds)\n",
      " block 1: loss 0.0359 (0:00:10.689288 seconds)\n",
      " block 9: loss 0.0359 (0:00:10.683752 seconds)\n",
      " block 17: loss 0.0359 (0:00:10.700024 seconds)\n",
      " block 25: loss 0.0359 (0:00:10.868084 seconds)\n",
      " block 33: loss 0.0359 (0:00:10.917799 seconds)\n",
      " block 41: loss 0.0359 (0:00:10.940415 seconds)\n",
      " block 49: loss 0.0358 (0:00:10.839433 seconds)\n",
      " block 57: loss 0.0358 (0:00:10.758624 seconds)\n",
      " block 2: loss 0.0358 (0:00:10.696905 seconds)\n",
      " block 10: loss 0.0358 (0:00:10.687562 seconds)\n",
      " block 26: loss 0.0358 (0:00:10.766142 seconds)\n",
      " block 34: loss 0.0359 (0:00:10.848960 seconds)\n",
      " block 42: loss 0.0358 (0:00:10.732031 seconds)\n",
      " block 50: loss 0.0358 (0:00:10.710119 seconds)\n",
      " block 18: loss 0.0358 (0:00:10.681079 seconds)\n",
      " block 58: loss 0.0358 (0:00:10.725190 seconds)\n",
      " block 3: loss 0.0358 (0:00:10.725416 seconds)\n",
      " block 11: loss 0.0358 (0:00:10.693098 seconds)\n",
      " block 19: loss 0.0358 (0:00:10.686961 seconds)\n",
      " block 27: loss 0.0359 (0:00:10.697532 seconds)\n",
      " block 35: loss 0.0360 (0:00:10.689394 seconds)\n",
      " block 51: loss 0.0361 (0:00:10.692326 seconds)\n",
      " block 43: loss 0.0360 (0:00:10.716449 seconds)\n",
      " block 59: loss 0.0360 (0:00:10.745978 seconds)\n",
      " block 20: loss 0.0362 (0:00:10.686603 seconds)\n",
      " block 28: loss 0.0363 (0:00:10.711145 seconds)\n",
      " block 4: loss 0.0363 (0:00:10.694227 seconds)\n",
      " block 12: loss 0.0364 (0:00:10.739195 seconds)\n",
      " block 36: loss 0.0365 (0:00:10.705214 seconds)\n",
      " block 44: loss 0.0365 (0:00:10.702832 seconds)\n",
      " block 5: loss 0.0365 (0:00:10.689531 seconds)\n",
      " block 60: loss 0.0365 (0:00:10.777411 seconds)\n",
      " block 21: loss 0.0365 (0:00:10.773811 seconds)\n",
      " block 29: loss 0.0365 (0:00:10.710489 seconds)\n",
      " block 13: loss 0.0364 (0:00:10.748534 seconds)\n",
      " block 37: loss 0.0365 (0:00:10.747215 seconds)\n",
      " block 45: loss 0.0364 (0:00:10.757977 seconds)\n",
      " block 53: loss 0.0364 (0:00:10.755802 seconds)\n",
      " block 61: loss 0.0363 (0:00:10.842002 seconds)\n",
      " block 6: loss 0.0363 (0:00:10.944691 seconds)\n",
      " block 14: loss 0.0363 (0:00:10.767753 seconds)\n",
      " block 22: loss 0.0363 (0:00:10.790920 seconds)\n",
      " block 52: loss 0.0363 (0:00:10.801889 seconds)\n",
      " block 30: loss 0.0363 (0:00:10.790602 seconds)\n",
      " block 38: loss 0.0364 (0:00:10.776789 seconds)\n",
      " block 46: loss 0.0363 (0:00:10.738469 seconds)\n",
      " block 54: loss 0.0363 (0:00:10.787933 seconds)\n",
      " block 62: loss 0.0363 (0:00:10.788698 seconds)\n",
      " block 7: loss 0.0362 (0:00:10.725527 seconds)\n",
      " block 15: loss 0.0362 (0:00:11.094106 seconds)\n",
      " block 23: loss 0.0362 (0:00:10.792094 seconds)\n",
      " block 31: loss 0.0362 (0:00:10.847883 seconds)\n",
      " block 39: loss 0.0362 (0:00:10.810458 seconds)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " block 47: loss 0.0361 (0:00:10.779556 seconds)\n",
      " block 55: loss 0.0361 (0:00:10.794535 seconds)\n",
      " block 63: loss 0.0361 (0:00:10.783276 seconds)\n",
      " epoch complete! 0:11:47.936144\n",
      "\n",
      "Epoch 8\n",
      " block 0: loss 0.0360 (0:00:10.834612 seconds)\n",
      " block 8: loss 0.0360 (0:00:10.755368 seconds)\n",
      " block 16: loss 0.0360 (0:00:10.764217 seconds)\n",
      " block 24: loss 0.0360 (0:00:10.779613 seconds)\n",
      " block 32: loss 0.0360 (0:00:10.723995 seconds)\n",
      " block 40: loss 0.0360 (0:00:10.696140 seconds)\n",
      " block 48: loss 0.0360 (0:00:10.679587 seconds)\n",
      " block 56: loss 0.0360 (0:00:10.704633 seconds)\n",
      " block 1: loss 0.0359 (0:00:10.621030 seconds)\n",
      " block 9: loss 0.0359 (0:00:10.647549 seconds)\n",
      " block 17: loss 0.0359 (0:00:10.671699 seconds)\n",
      " block 25: loss 0.0359 (0:00:10.846037 seconds)\n",
      " block 33: loss 0.0359 (0:00:10.721404 seconds)\n",
      " block 41: loss 0.0359 (0:00:10.656352 seconds)\n",
      " block 49: loss 0.0359 (0:00:10.660742 seconds)\n",
      " block 57: loss 0.0359 (0:00:10.700748 seconds)\n",
      " block 2: loss 0.0358 (0:00:10.645864 seconds)\n",
      " block 10: loss 0.0358 (0:00:10.633820 seconds)\n",
      " block 26: loss 0.0358 (0:00:10.637928 seconds)\n",
      " block 34: loss 0.0359 (0:00:10.657732 seconds)\n",
      " block 42: loss 0.0359 (0:00:10.661758 seconds)\n",
      " block 50: loss 0.0359 (0:00:10.639635 seconds)\n",
      " block 18: loss 0.0359 (0:00:10.626095 seconds)\n",
      " block 58: loss 0.0358 (0:00:10.684077 seconds)\n",
      " block 3: loss 0.0358 (0:00:10.640719 seconds)\n",
      " block 11: loss 0.0359 (0:00:10.673636 seconds)\n",
      " block 19: loss 0.0359 (0:00:10.628066 seconds)\n",
      " block 27: loss 0.0359 (0:00:10.654070 seconds)\n",
      " block 35: loss 0.0361 (0:00:10.685560 seconds)\n",
      " block 51: loss 0.0361 (0:00:10.743597 seconds)\n",
      " block 43: loss 0.0361 (0:00:10.614306 seconds)\n",
      " block 59: loss 0.0361 (0:00:10.774971 seconds)\n",
      " block 20: loss 0.0361 (0:00:10.651799 seconds)\n",
      " block 28: loss 0.0362 (0:00:10.631011 seconds)\n",
      " block 4: loss 0.0363 (0:00:10.603318 seconds)\n",
      " block 12: loss 0.0363 (0:00:10.590654 seconds)\n",
      " block 36: loss 0.0364 (0:00:10.898030 seconds)\n",
      " block 44: loss 0.0365 (0:00:10.600726 seconds)\n",
      " block 5: loss 0.0364 (0:00:10.598303 seconds)\n",
      " block 60: loss 0.0364 (0:00:10.774911 seconds)\n",
      " block 21: loss 0.0364 (0:00:10.664283 seconds)\n",
      " block 29: loss 0.0364 (0:00:10.587894 seconds)\n",
      " block 13: loss 0.0364 (0:00:10.630563 seconds)\n",
      " block 37: loss 0.0364 (0:00:10.583417 seconds)\n",
      " block 45: loss 0.0364 (0:00:10.814569 seconds)\n",
      " block 53: loss 0.0363 (0:00:10.660197 seconds)\n",
      " block 61: loss 0.0363 (0:00:10.725205 seconds)\n",
      " block 6: loss 0.0363 (0:00:10.688020 seconds)\n",
      " block 14: loss 0.0362 (0:00:10.747947 seconds)\n",
      " block 22: loss 0.0362 (0:00:10.660596 seconds)\n",
      " block 52: loss 0.0363 (0:00:10.734591 seconds)\n",
      " block 30: loss 0.0363 (0:00:10.688723 seconds)\n",
      " block 38: loss 0.0363 (0:00:10.676602 seconds)\n",
      " block 46: loss 0.0363 (0:00:10.640441 seconds)\n",
      " block 54: loss 0.0363 (0:00:10.713387 seconds)\n",
      " block 62: loss 0.0362 (0:00:10.682706 seconds)\n",
      " block 7: loss 0.0362 (0:00:10.756258 seconds)\n",
      " block 15: loss 0.0362 (0:00:10.673012 seconds)\n",
      " block 23: loss 0.0361 (0:00:10.887861 seconds)\n",
      " block 31: loss 0.0361 (0:00:10.772126 seconds)\n",
      " block 39: loss 0.0362 (0:00:10.917365 seconds)\n",
      " block 47: loss 0.0361 (0:00:10.613491 seconds)\n",
      " block 55: loss 0.0361 (0:00:10.577915 seconds)\n",
      " block 63: loss 0.0361 (0:00:10.679267 seconds)\n",
      " epoch complete! 0:11:43.225662\n",
      "\n",
      "Epoch 9\n",
      " block 0: loss 0.0360 (0:00:10.702863 seconds)\n",
      " block 8: loss 0.0360 (0:00:10.637338 seconds)\n",
      " block 16: loss 0.0360 (0:00:10.721507 seconds)\n",
      " block 24: loss 0.0360 (0:00:10.690543 seconds)\n",
      " block 32: loss 0.0360 (0:00:10.908531 seconds)\n",
      " block 40: loss 0.0360 (0:00:10.710087 seconds)\n",
      " block 48: loss 0.0360 (0:00:10.657286 seconds)\n",
      " block 56: loss 0.0360 (0:00:10.692909 seconds)\n",
      " block 1: loss 0.0359 (0:00:10.684034 seconds)\n",
      " block 9: loss 0.0359 (0:00:10.709215 seconds)\n",
      " block 17: loss 0.0359 (0:00:10.671611 seconds)\n",
      " block 25: loss 0.0359 (0:00:10.673402 seconds)\n",
      " block 33: loss 0.0359 (0:00:10.623844 seconds)\n",
      " block 41: loss 0.0359 (0:00:10.635952 seconds)\n",
      " block 49: loss 0.0359 (0:00:10.638616 seconds)\n",
      " block 57: loss 0.0359 (0:00:10.672239 seconds)\n",
      " block 2: loss 0.0359 (0:00:10.731264 seconds)\n",
      " block 10: loss 0.0358 (0:00:10.637379 seconds)\n",
      " block 26: loss 0.0359 (0:00:10.647866 seconds)\n",
      " block 34: loss 0.0359 (0:00:10.632134 seconds)\n",
      " block 42: loss 0.0359 (0:00:10.631319 seconds)\n",
      " block 50: loss 0.0359 (0:00:10.632540 seconds)\n",
      " block 18: loss 0.0359 (0:00:10.694469 seconds)\n",
      " block 58: loss 0.0358 (0:00:10.843686 seconds)\n",
      " block 3: loss 0.0359 (0:00:10.697592 seconds)\n",
      " block 11: loss 0.0359 (0:00:10.633140 seconds)\n",
      " block 19: loss 0.0359 (0:00:10.635424 seconds)\n",
      " block 27: loss 0.0359 (0:00:10.948743 seconds)\n",
      " block 35: loss 0.0360 (0:00:10.624404 seconds)\n",
      " block 51: loss 0.0360 (0:00:10.657566 seconds)\n",
      " block 43: loss 0.0360 (0:00:10.641281 seconds)\n",
      " block 59: loss 0.0360 (0:00:10.668104 seconds)\n",
      " block 20: loss 0.0361 (0:00:10.809036 seconds)\n",
      " block 28: loss 0.0362 (0:00:10.676289 seconds)\n",
      " block 4: loss 0.0362 (0:00:10.737889 seconds)\n",
      " block 12: loss 0.0363 (0:00:10.677708 seconds)\n",
      " block 36: loss 0.0364 (0:00:10.655597 seconds)\n",
      " block 44: loss 0.0364 (0:00:10.648786 seconds)\n",
      " block 5: loss 0.0364 (0:00:10.699819 seconds)\n",
      " block 60: loss 0.0364 (0:00:10.675950 seconds)\n",
      " block 21: loss 0.0364 (0:00:10.561735 seconds)\n",
      " block 29: loss 0.0364 (0:00:10.618902 seconds)\n",
      " block 13: loss 0.0363 (0:00:10.623191 seconds)\n",
      " block 37: loss 0.0364 (0:00:10.587426 seconds)\n",
      " block 45: loss 0.0363 (0:00:10.609561 seconds)\n",
      " block 53: loss 0.0363 (0:00:10.596294 seconds)\n",
      " block 61: loss 0.0363 (0:00:10.748192 seconds)\n",
      " block 6: loss 0.0362 (0:00:10.612646 seconds)\n",
      " block 14: loss 0.0362 (0:00:10.985346 seconds)\n",
      " block 22: loss 0.0362 (0:00:10.647149 seconds)\n",
      " block 52: loss 0.0362 (0:00:10.664450 seconds)\n",
      " block 30: loss 0.0363 (0:00:10.821452 seconds)\n",
      " block 38: loss 0.0363 (0:00:10.638450 seconds)\n",
      " block 46: loss 0.0363 (0:00:10.632107 seconds)\n",
      " block 54: loss 0.0362 (0:00:10.725679 seconds)\n",
      " block 62: loss 0.0362 (0:00:10.688974 seconds)\n",
      " block 7: loss 0.0362 (0:00:10.715315 seconds)\n",
      " block 15: loss 0.0362 (0:00:10.727828 seconds)\n",
      " block 23: loss 0.0361 (0:00:10.702981 seconds)\n",
      " block 31: loss 0.0361 (0:00:10.725136 seconds)\n",
      " block 39: loss 0.0361 (0:00:10.917331 seconds)\n",
      " block 47: loss 0.0361 (0:00:10.835402 seconds)\n",
      " block 55: loss 0.0361 (0:00:10.697157 seconds)\n",
      " block 63: loss 0.0360 (0:00:10.687117 seconds)\n",
      " epoch complete! 0:11:43.507125\n",
      "\n",
      "Epoch 10\n",
      " block 0: loss 0.0360 (0:00:10.655211 seconds)\n",
      " block 8: loss 0.0360 (0:00:10.627240 seconds)\n",
      " block 16: loss 0.0360 (0:00:10.623363 seconds)\n",
      " block 24: loss 0.0360 (0:00:10.644739 seconds)\n",
      " block 32: loss 0.0360 (0:00:10.734969 seconds)\n",
      " block 40: loss 0.0360 (0:00:10.692758 seconds)\n",
      " block 48: loss 0.0360 (0:00:10.648569 seconds)\n",
      " block 56: loss 0.0360 (0:00:10.713191 seconds)\n",
      " block 1: loss 0.0359 (0:00:10.659516 seconds)\n",
      " block 9: loss 0.0359 (0:00:10.715361 seconds)\n",
      " block 17: loss 0.0359 (0:00:10.662131 seconds)\n",
      " block 25: loss 0.0359 (0:00:10.673037 seconds)\n",
      " block 33: loss 0.0359 (0:00:10.689936 seconds)\n",
      " block 41: loss 0.0359 (0:00:10.645970 seconds)\n",
      " block 49: loss 0.0359 (0:00:10.681269 seconds)\n",
      " block 57: loss 0.0359 (0:00:10.848695 seconds)\n",
      " block 2: loss 0.0359 (0:00:10.653475 seconds)\n",
      " block 10: loss 0.0359 (0:00:10.659245 seconds)\n",
      " block 26: loss 0.0359 (0:00:10.639347 seconds)\n",
      " block 34: loss 0.0359 (0:00:10.648909 seconds)\n",
      " block 42: loss 0.0359 (0:00:10.656923 seconds)\n",
      " block 50: loss 0.0359 (0:00:10.664539 seconds)\n",
      " block 18: loss 0.0359 (0:00:10.715786 seconds)\n",
      " block 58: loss 0.0359 (0:00:10.729968 seconds)\n",
      " block 3: loss 0.0359 (0:00:10.734717 seconds)\n",
      " block 11: loss 0.0359 (0:00:10.819061 seconds)\n",
      " block 19: loss 0.0359 (0:00:10.684465 seconds)\n",
      " block 27: loss 0.0360 (0:00:10.652455 seconds)\n",
      " block 35: loss 0.0360 (0:00:10.667763 seconds)\n",
      " block 51: loss 0.0361 (0:00:10.678202 seconds)\n",
      " block 43: loss 0.0361 (0:00:10.641932 seconds)\n",
      " block 59: loss 0.0361 (0:00:10.695283 seconds)\n",
      " block 20: loss 0.0361 (0:00:10.672548 seconds)\n",
      " block 28: loss 0.0362 (0:00:10.682666 seconds)\n",
      " block 4: loss 0.0362 (0:00:10.702486 seconds)\n",
      " block 12: loss 0.0363 (0:00:10.688195 seconds)\n",
      " block 36: loss 0.0364 (0:00:10.685963 seconds)\n",
      " block 44: loss 0.0364 (0:00:10.659823 seconds)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " block 5: loss 0.0363 (0:00:10.637651 seconds)\n",
      " block 60: loss 0.0364 (0:00:10.847859 seconds)\n",
      " block 21: loss 0.0363 (0:00:10.694068 seconds)\n",
      " block 29: loss 0.0363 (0:00:10.718661 seconds)\n",
      " block 13: loss 0.0363 (0:00:10.831230 seconds)\n",
      " block 37: loss 0.0363 (0:00:11.122896 seconds)\n",
      " block 45: loss 0.0363 (0:00:10.728456 seconds)\n",
      " block 53: loss 0.0363 (0:00:10.665743 seconds)\n",
      " block 61: loss 0.0362 (0:00:10.721676 seconds)\n",
      " block 6: loss 0.0362 (0:00:10.685600 seconds)\n",
      " block 14: loss 0.0362 (0:00:10.747670 seconds)\n",
      " block 22: loss 0.0362 (0:00:10.678536 seconds)\n",
      " block 52: loss 0.0362 (0:00:10.703445 seconds)\n",
      " block 30: loss 0.0362 (0:00:10.712526 seconds)\n",
      " block 38: loss 0.0363 (0:00:10.702869 seconds)\n",
      " block 46: loss 0.0362 (0:00:10.793621 seconds)\n",
      " block 54: loss 0.0362 (0:00:10.676423 seconds)\n",
      " block 62: loss 0.0362 (0:00:10.711857 seconds)\n",
      " block 7: loss 0.0362 (0:00:10.695829 seconds)\n",
      " block 15: loss 0.0361 (0:00:10.703081 seconds)\n",
      " block 23: loss 0.0361 (0:00:10.677297 seconds)\n",
      " block 31: loss 0.0361 (0:00:10.713909 seconds)\n",
      " block 39: loss 0.0361 (0:00:10.698594 seconds)\n",
      " block 47: loss 0.0361 (0:00:10.733807 seconds)\n",
      " block 55: loss 0.0361 (0:00:10.706192 seconds)\n",
      " block 63: loss 0.0360 (0:00:10.702748 seconds)\n",
      " epoch complete! 0:11:43.692215\n",
      "\n",
      "Epoch 11\n",
      " block 0: loss 0.0360 (0:00:10.693854 seconds)\n",
      " block 8: loss 0.0360 (0:00:10.633332 seconds)\n",
      " block 16: loss 0.0360 (0:00:10.732488 seconds)\n",
      " block 24: loss 0.0360 (0:00:10.889973 seconds)\n",
      " block 32: loss 0.0360 (0:00:10.704453 seconds)\n",
      " block 40: loss 0.0360 (0:00:10.620255 seconds)\n",
      " block 48: loss 0.0360 (0:00:10.819238 seconds)\n",
      " block 56: loss 0.0360 (0:00:10.782870 seconds)\n",
      " block 1: loss 0.0359 (0:00:10.624717 seconds)\n",
      " block 9: loss 0.0359 (0:00:10.652593 seconds)\n",
      " block 17: loss 0.0359 (0:00:10.633185 seconds)\n",
      " block 25: loss 0.0359 (0:00:10.630688 seconds)\n",
      " block 33: loss 0.0359 (0:00:10.668469 seconds)\n",
      " block 41: loss 0.0359 (0:00:10.685790 seconds)\n",
      " block 49: loss 0.0359 (0:00:10.641075 seconds)\n",
      " block 57: loss 0.0359 (0:00:10.713298 seconds)\n",
      " block 2: loss 0.0359 (0:00:10.846199 seconds)\n",
      " block 10: loss 0.0359 (0:00:10.728821 seconds)\n",
      " block 26: loss 0.0359 (0:00:10.718349 seconds)\n",
      " block 34: loss 0.0359 (0:00:10.689760 seconds)\n",
      " block 42: loss 0.0359 (0:00:10.605731 seconds)\n",
      " block 50: loss 0.0359 (0:00:10.615479 seconds)\n",
      " block 18: loss 0.0359 (0:00:10.634529 seconds)\n",
      " block 58: loss 0.0359 (0:00:10.653308 seconds)\n",
      " block 3: loss 0.0359 (0:00:10.647191 seconds)\n",
      " block 11: loss 0.0359 (0:00:10.655782 seconds)\n",
      " block 19: loss 0.0359 (0:00:10.637879 seconds)\n",
      " block 27: loss 0.0360 (0:00:10.648391 seconds)\n",
      " block 35: loss 0.0361 (0:00:10.621205 seconds)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-171-339c636bd4e0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_iters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m             \u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNwc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m             \u001b[0mresults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/anaconda/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    578\u001b[0m         \u001b[0mxla_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 580\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    581\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    582\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/anaconda/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    609\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    610\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 611\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    612\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    613\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/anaconda/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2418\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2419\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2420\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2422\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/anaconda/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   1663\u001b[0m          if isinstance(t, (ops.Tensor,\n\u001b[1;32m   1664\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[0;32m-> 1665\u001b[0;31m         self.captured_inputs)\n\u001b[0m\u001b[1;32m   1666\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1667\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/anaconda/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1744\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1745\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1746\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1747\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1748\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/anaconda/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    596\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    597\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 598\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    599\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    600\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m/opt/conda/anaconda/lib/python3.7/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# instantiate the model\n",
    "model = MFEmbedder(31986, 300, 8)\n",
    "\n",
    "# make the training loss and training step functions\n",
    "train_loss, train_step = make_train_step(_W, _C, N, k)\n",
    "\n",
    "results = []\n",
    "n_iters = 100\n",
    "\n",
    "t0 = dt.utcnow()\n",
    "for e in range(50):\n",
    "    print(f\"Epoch {e}\")\n",
    "    e0 = dt.utcnow()\n",
    "    # iterate through blocks keeping one partition in memory at a time\n",
    "    for block_entries in entries.mapPartitions(lambda x: [list(x)]).toLocalIterator():\n",
    "        \n",
    "        b0 = dt.utcnow()\n",
    "        \n",
    "        # convert to numpy array\n",
    "        block_entries = np.array(block_entries, float)\n",
    "\n",
    "        # choose a random entry to work out which block we have (could be any entry)\n",
    "        w, c = block_entries[np.random.choice(range(len(block_entries))), :2]\n",
    "        b = model._get_block_from_index(w, c)\n",
    "\n",
    "        # we need the indices ranges so compute min and max\n",
    "        min_ = block_entries.min(0)\n",
    "        max_ = block_entries.max(0)\n",
    "\n",
    "        # reformat our list of entries into a dense matrix\n",
    "        Nwc = formatNwc(block_entries, \n",
    "                        int(max_[0]+1-min_[0]),\n",
    "                        int(max_[1]+1-min_[1]))\n",
    "\n",
    "        # extract the relevant parts of the marginalised counts\n",
    "        Nw = _W[int(min_[0]):int(max_[0]+1)]\n",
    "        Nc = _C[int(min_[1]):int(max_[1]+1)]\n",
    "\n",
    "        for i in range(n_iters): \n",
    "\n",
    "            # train the model\n",
    "            train_step(model, b, Nw, Nc, Nwc)\n",
    "            res = train_loss.result()  \n",
    "            results.append(res)\n",
    "\n",
    "        print(f\" block {b}: loss {res:0.4f} ({str(dt.utcnow()-b0)} seconds)\")\n",
    "    print(f\" epoch complete! {str(dt.utcnow()-e0)}\\n\")            \n",
    "print(f\"\\nTotal time: {str(dt.utcnow()-t0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract W as the model variable\n",
    "W = model.W.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(12, 5))\n",
    "ax.plot(range(len(results)), results)\n",
    "for spine in [\"top\", \"right\"]:\n",
    "    ax.spines[spine].set_visible(False)  \n",
    "    ax.spines[spine].set_visible(False)\n",
    "ax.set_ylabel(\"Loss\", rotation=90, size=15)\n",
    "ax.set_xlabel(\"Training Epoch\", size=15)\n",
    "ax.set_title(\"Implicit Matrix Factorisation Training\", size=20)\n",
    "plt.xticks([64*i for i in range(11)], labels=range(11))\n",
    "\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's save the embedding to text. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs = gcsfs.GCSFileSystem(project=\"ST446-project-1\", token=\"cloud\")\n",
    "with fs.open(\"gs://fin-bucket/SGNS/Wimp_1.txt\", \"w\") as f:\n",
    "    for l in W:\n",
    "        f.write(\",\".join([str(i) for i in row])+\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "****\n",
    "# Embedding Results\n",
    "\n",
    "Here let's test some of the embeddings. First let's load them up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec = Word2VecModel.load(\"gs://fin-bucket/nwe/w2v_1\")\n",
    "\n",
    "fs = gcsfs.GCSFileSystem(project=\"ST446-project-1\", token=\"cloud\")\n",
    "with fs.open(\"gs://fin-bucket/SGNS/Wexp_1.txt\", \"r\") as f:\n",
    "    explicit = np.genfromtxt(f, delimiter=\",\", dtype=int)\n",
    "    \n",
    "fs = gcsfs.GCSFileSystem(project=\"ST446-project-1\", token=\"cloud\")\n",
    "with fs.open(\"gs://fin-bucket/SGNS/Wimp_1.txt\", \"r\") as f:\n",
    "    implicit = np.genfromtxt(f, delimiter=\",\", dtype=int)\n",
    "\n",
    "fs = gcsfs.GCSFileSystem(project=\"ST446-project-1\", token=\"cloud\")\n",
    "with fs.open(\"gs://fin-bucket/SGNS/w2i_1.txt\", \"r\") as f:\n",
    "    w2i = np.genfromtxt(f, delimiter=\",\", dtype=int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's define some functions that will allow us to check some of the embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ash', 0.8485072834), ('shoot', 0.7395426211), ('serbia', 0.7259330585)]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def cossim(a, b, norma=None):\n",
    "    # compute cosine similarity\n",
    "    if norma is None:\n",
    "        norma = np.linalg.norm(a)\n",
    "    return np.dot(a, b) / (norma * np.linalg.norm(b))\n",
    "\n",
    "def get_n_similar(word, n, W):\n",
    "    \n",
    "    # Find n most similar words in W.\n",
    "    \n",
    "    # get the embedded vector for word and compute its norm\n",
    "    w = W[w2i.index(word)]\n",
    "    normv = np.linalg.norm(w)\n",
    "   \n",
    "    # instantiate lists to hold the indices of top n closest\n",
    "    indices = [-1] * n\n",
    "    cossims = [-np.inf] * n\n",
    "    cut = -np.inf\n",
    "    \n",
    "    # loop through embedded vectors\n",
    "    for i, v in enumerate(W):\n",
    "        c = cossim(w, v, normv)\n",
    "        # if cossim is greater than the cutoff (but is not the same word)...\n",
    "        if 1 > c > cut:\n",
    "            # insert i, c into the index and cossims lists\n",
    "            j = bisect_left(cossims, c)\n",
    "            cossims.insert(j, c)\n",
    "            cossims.pop(0)\n",
    "            indices.insert(j, i)\n",
    "            indices.pop(0)\n",
    "            cut = cossims[0]\n",
    "    # return top n converted back to words\n",
    "    return [(w2i[i], c) for i, c in zip(indices, cossims)]\n",
    "\n",
    "get_n_similar(\"girl\", 3, implicit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Most similar words:\n",
      "              |   word2vec   |   explicit   |   implicit   |\n",
      "--------------+--------------+--------------+--------------+\n",
      "              |   baseball   |   stadium    |    happy     |\n",
      "   football   |    rugby     |    beach     |    sandal    |\n",
      "              |   stadium    |   baseball   |    lemon     |\n",
      "--------------+--------------+--------------+--------------+\n",
      "              |    eight     |     cafe     |   computer   |\n",
      "    seven     |     nine     |     nine     |    paint     |\n",
      "              |     week     |    spain     |     walk     |\n",
      "--------------+--------------+--------------+--------------+\n",
      "              |    queen     |    court     |    center    |\n",
      "     king     |    prince    |    royal     |   district   |\n",
      "              |     lord     |    prince    |    maple     |\n",
      "--------------+--------------+--------------+--------------+\n",
      "              |    london    |    london    |    recent    |\n",
      "  manchester  |    leeds     |    north     | declaration  |\n",
      "              |   arsenal    |    europe    |   address    |\n",
      "--------------+--------------+--------------+--------------+\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_words = [\"football\", \"seven\", \"king\", \"manchester\"]\n",
    "\n",
    "print(\"Most similar words:\")\n",
    "print(\"              |   word2vec   |   explicit   |   implicit   |\")\n",
    "print(\"--------------+--------------+--------------+--------------+\")\n",
    "for word in test_words:\n",
    "    print(\"\\n\".join([f\"{w:^14}|{w2v:^14}|{exp:^14}|{imp:^14}|\" for w, w2v, exp, imp in \n",
    "                    zip([\"\", word, \"\"],\n",
    "                        get_n_similar(word, 3, word2vec),\n",
    "                        get_n_similar(word, 3, implicit),\n",
    "                        get_n_similar(word, 3, explicit)\n",
    "                       )\n",
    "                    ]))\n",
    "    print(\"--------------+--------------+--------------+--------------+\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The word2vec embeddings produce similarities that one might expect. The explicit embedding is not far off and even has some of the same words come up as the word2vec. As we might have guess from the loss the implicit embeddings come up with some nonsensical similarities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
